{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "e8fd100a2c8866ae2d3e8b85cd01e709724ac0af43a714c586c6afac94a5cb88"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "source": [
    "创建一个张量，设置`requires_grad=True`来跟踪与它相关的计算"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "source": [
    "针对此张量做一个操作"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]], grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "y = x + 2\n",
    "y"
   ]
  },
  {
   "source": [
    "y作为操作结果被创建，所以它有`grad_fn`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x1de07f9b5c8>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "source": [
    "针对`y`做更多的操作"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[27., 27.],\n",
       "        [27., 27.]], grad_fn=<MulBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(27., grad_fn=<MeanBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "out = z.mean()\n",
    "out"
   ]
  },
  {
   "source": [
    "我们知道后向传播（Backward propogation）是用来算梯度的，那我们以上面的`out`为例来看一下梯度的计算结果。由于`out`这个输出只是一个标量，以下的`out.backward()`等同于`out.backward(torch.tensor(1.))`。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "source": [
    "打印`out`关于`x`的梯度"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[4.5000, 4.5000],\n",
       "        [4.5000, 4.5000]])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "source": [
    "由于`out=1/4*sum_{i=1, j=1}^{2, 2} 3*(x_{i,j} + 2)^2`,所以以上的`\\partial out/ \\partial x_{i,j}`的计算结果正确"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "`.requires_grad_()`会改变张量的`requires_grad`标记。如果没有提供相应参数，则默认为`False`。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = (a * 3) / (a - 1)\n",
    "a.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "a.requires_grad_(True)\n",
    "a.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(True, <SumBackward0 at 0x1de0d3b7cc8>)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "b = (a * a).sum()\n",
    "b.requires_grad, b.grad_fn"
   ]
  },
  {
   "source": [
    "接下来看一个雅可比向量积的例子"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.3374, 1.8801, 2.3572], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.6748, 3.7603, 4.7143], grad_fn=<MulBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "y = x * 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 172.7489,  962.6292, 1206.8728], grad_fn=<MulBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "while y.data.norm()<1000:\n",
    "    y = y * 2\n",
    "\n",
    "y"
   ]
  },
  {
   "source": [
    "现在，`y`不再是一个标量。`torch.autograd`不能够直接计算整个雅可比，但是我们可以计算雅可比向量积（见与本程序相关的md文档），只需要简单地传递向量给backward()作为参数。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "x.grad"
   ]
  },
  {
   "source": [
    "可以通过将代码包裹在`with torch.no_grad()`，来停止对从跟踪历史中的`.requires_grad=True`的张量自动求导。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nTrue\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  }
 ]
}